{"cells":[{"cell_type":"markdown","metadata":{"id":"IkSguVy8Xv83"},"source":["# **2D segmentation with a U-Net**\n","---\n","#### Bioimage analysis course, Gothenburg University, September 2022\n","#### Daniel Sage, EPFL, Lausanne, Switzerland\n","#### Anaïs Badoual, Inria, France\n","---\n","\n","This notebook is a modified version of the **U-Net 2D** notebook of the Zero-CostDL4Mic (https://github.com/HenriquesLab/DeepLearning_Collab/wiki). This goal of this notebook is to train a neural network U-net for an image  segmentation task by a pixel classification (only two classes, here). The model is stored in the Bioimage Model Zoo format and directly usable in deepImageJ. \n","\n","---\n","**References**\n","- [U-Net](https://arxiv.org/abs/1505.04597) Ronneberger et al. MICCAI 2015.\n","- [ZeroCostDL4Mic](https://www.nature.com/articles/s41467-021-22518-0): L. von Chamier et al., Nature Methods, 2021. Developed by the [G. Jacquemet Lab](https://cellmig.org/) and [R. Henriques Lab](https://henriqueslab.github.io/).\n","- [Bioimage Model Zoo (BMZ)](https://www.biorxiv.org/content/10.1101/2022.06.07.495102v1) W. Ouyang et al., biorxiv  2022.\n","- [BMZ exporter](https://github.com/esgomezm): E. Gómez-de-Mariscal 2021.\n","- [DeepImageJ](https://doi.org/10.1038/s41592-021-01262-9): E. Gómez-de-Mariscal et al., Nature Methods 2021. deepImage team\n","\n","*Please also cite this original paper when using or developing this notebook.* "]},{"cell_type":"markdown","metadata":{"id":"eHpfWKZIPf6l"},"source":["# **1. Initialization**\n","\n","To clean all outputs, use the command of the menu: Edit -> Clear all outputs\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"uc0haIa-fZiG"},"outputs":[],"source":["#@markdown ### **1.1 Install TensorFlow 1.15 and other libraries**\n","# New running TF1.15 on Colab 2022\n","!pip uninstall -y -q tensorflow\n","!pip install -q tensorflow-probability==0.8\n","!pip install -q kapre==0.1.7\n","!pip install -q tensorflow==1.15\n","# Install packages which are not included in Google Colab\n","\n","!pip install data\n","!pip install -q tifffile # contains tools to operate tiff-files\n","#!pip install edt # improves STARDIST performances\n","!pip install -q wget\n","!pip install -q fpdf\n","!pip install -q PTable # Nice tables \n","!pip install -q zarr\n","!pip install -q imagecodecs\n","!pip install h5py==2.10\n","!pip install \"bioimageio.core>=0.5,<0.6\"\n","!pip install pyyaml==5.4.1\n","#!pip uninstall -y -q keras-nightly\n","\n","!pip install codecarbon\n","#Force session restart\n","exit(0)\n","\n","from IPython. display import clear_output\n","clear_output(wait=False)\n","\n","print(\"The required packages are installed.\")\n"]},{"cell_type":"markdown","metadata":{"id":"mx21dM6nT6rP"},"source":["Ignore the following message error message. Your Runtime has automatically restarted. This is normal.\n","\n","<img width=\"40%\" alt =\"\" src=\"https://github.com/HenriquesLab/ZeroCostDL4Mic/raw/master/Wiki_files/session_crash.png\"><figcaption>  </figcaption>"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"tLI8lOQqJq52"},"outputs":[],"source":["from __future__ import print_function\n","\n","# Suppressing some warnings\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from builtins import any as b_any\n","import sys\n","before = [str(m) for m in sys.modules]\n","\n","Notebook_version = \"1.2\"\n","#@markdown ### **1.2 Import packages and define functions**\n","\n","import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n","\n","#%tensorflow_version 1.x\n","import tensorflow as tf\n","print('TensorFlow Version: ', tf.__version__)\n","tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n","\n","\n","# Keras imports\n","from tensorflow.keras import models\n","from tensorflow.keras.models import Model, load_model\n","from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, concatenate, UpSampling2D\n","from tensorflow.keras.optimizers import Adam\n","# from keras.callbacks import ModelCheckpoint, LearningRateScheduler, CSVLogger # we currently don't use any other callbacks from ModelCheckpoints\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.callbacks import ReduceLROnPlateau\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n","from tensorflow.keras import backend as keras\n","\n","# General import\n","import numpy as np\n","import pandas as pd\n","import glob\n","from skimage import img_as_ubyte, img_as_float32, io, transform\n","import matplotlib as mpl\n","from matplotlib import pyplot as plt\n","from matplotlib.pyplot import imread\n","from pathlib import Path\n","import shutil\n","import random\n","import time\n","import csv\n","import sys\n","from math import ceil\n","from fpdf import FPDF, HTMLMixin\n","from pip._internal.operations.freeze import freeze\n","import subprocess\n","\n","# Imports for QC\n","from PIL import Image\n","from scipy import signal\n","from scipy import ndimage\n","from sklearn.linear_model import LinearRegression\n","from skimage.util import img_as_uint\n","from skimage.metrics import structural_similarity\n","from skimage.metrics import peak_signal_noise_ratio as psnr\n","\n","# For sliders and dropdown menu and progress bar\n","from ipywidgets import interact\n","import ipywidgets as widgets\n","# from tqdm import tqdm\n","from tqdm.notebook import tqdm\n","\n","from sklearn.feature_extraction import image\n","from skimage import img_as_ubyte, io, transform\n","from skimage.util.shape import view_as_windows\n","from datetime import datetime\n","\n","# BioImage Model Zoo dependencies\n","from bioimageio.core import load_raw_resource_description, load_resource_description\n","from zipfile import ZipFile\n","from shutil import rmtree\n","from bioimageio.core.build_spec import build_model, add_weights\n","from bioimageio.core.resource_tests import test_model\n","from bioimageio.core.weight_converter.keras import convert_weights_to_tensorflow_saved_model_bundle\n","import requests\n","\n","# Deepiction\n","from codecarbon import EmissionsTracker\n","from google.colab import data_table\n","\n","Network = \"U-net\"\n","\n","def get_requirements_path():\n","    # Store requirements file in 'contents' directory \n","    current_dir = os.getcwd()\n","    dir_count = current_dir.count('/') - 1\n","    path = '../' * (dir_count) + 'requirements.txt'\n","    return path\n","\n","def filter_files(file_list, filter_list):\n","    filtered_list = []\n","    for fname in file_list:\n","        if b_any(fname.split('==')[0] in s for s in filter_list):\n","            filtered_list.append(fname)\n","    return filtered_list\n","\n","def build_requirements_file(before, after):\n","    path = get_requirements_path()\n","\n","    # Exporting requirements.txt for local run\n","    !pip freeze > $path\n","\n","    # Get minimum requirements file\n","    df = pd.read_csv(path, delimiter = \"\\n\")\n","    mod_list = [m.split('.')[0] for m in after if not m in before]\n","    req_list_temp = df.values.tolist()\n","    req_list = [x[0] for x in req_list_temp]\n","\n","    # Replace with package name and handle cases where import name is different to module name\n","    mod_name_list = [['sklearn', 'scikit-learn'], ['skimage', 'scikit-image']]\n","    mod_replace_list = [[x[1] for x in mod_name_list] if s in [x[0] for x in mod_name_list] else s for s in mod_list] \n","    filtered_list = filter_files(req_list, mod_replace_list)\n","\n","    file=open(path,'w')\n","    for item in filtered_list:\n","        file.writelines(item + '\\n')\n","\n","    file.close()\n","\n","from builtins import any as b_any\n","\n","def getClassWeights(Training_target_path):\n","\n","    \n","\n","\n","  Mask_dir_list = os.listdir(Training_target_path)\n","  number_of_dataset = len(Mask_dir_list)\n","  #print(\"number of mask:\", Mask_dir_list)\n","  class_count = np.zeros(2, dtype=int)\n","  #for i in tqdm(range(number_of_dataset)):\n","  for i in range(number_of_dataset):\n","    file = Mask_dir_list[i]\n","    if file.startswith('.'): \n","      print('Rejected ', file)\n","    elif file.endswith('.tif') or file.endswith('.tiff') or file.endswith('.png'): \n","      mask = io.imread(os.path.join(Training_target_path, file))\n","      mask = normalizeMinMax(mask)\n","      class_count[0] += mask.shape[0]*mask.shape[1] - mask.sum()\n","      class_count[1] += mask.sum()\n","\n","  n_samples = class_count.sum()\n","  n_classes = 2\n","  class_weights = n_samples / (n_classes * class_count)\n","  return class_weights\n","\n","def weighted_binary_crossentropy(class_weights):\n","\n","    def _weighted_binary_crossentropy(y_true, y_pred):\n","        binary_crossentropy = keras.binary_crossentropy(y_true, y_pred)\n","        weight_vector = y_true * class_weights[1] + (1. - y_true) * class_weights[0]\n","        weighted_binary_crossentropy = weight_vector * binary_crossentropy\n","        return keras.mean(weighted_binary_crossentropy)\n","\n","    return _weighted_binary_crossentropy\n","\n","def save_augment(datagen,orig_img,dir_augmented_data=\"/content/augment\"):\n","  \"\"\"\n","  Saves a subset of the augmented data for visualisation, by default in /content.\n","\n","  This is adapted from: https://fairyonice.github.io/Learn-about-ImageDataGenerator.html\n","  \n","  \"\"\"\n","  try:\n","    os.mkdir(dir_augmented_data)\n","  except:\n","        ## if the preview folder exists, then remove\n","        ## the contents (pictures) in the folder\n","    for item in os.listdir(dir_augmented_data):\n","      os.remove(dir_augmented_data + \"/\" + item)\n","\n","    ## convert the original image to array\n","  x = img_to_array(orig_img)\n","    ## reshape (Sampke, Nrow, Ncol, 3) 3 = R, G or B\n","    #print(x.shape)\n","  x = x.reshape((1,) + x.shape)\n","    #print(x.shape)\n","    ## -------------------------- ##\n","    ## randomly generate pictures\n","    ## -------------------------- ##\n","  i = 0\n","    #We will just save 5 images,\n","    #but this can be changed, but note the visualisation in 3. currently uses 5.\n","  Nplot = 5\n","  for batch in datagen.flow(x,batch_size=1,\n","                            save_to_dir=dir_augmented_data,\n","                            save_format='tif',\n","                            seed=42):\n","    i += 1\n","    if i > Nplot - 1:\n","      break\n","\n","# Generators\n","def buildDoubleGenerator(image_datagen, mask_datagen, image_folder_path, mask_folder_path, subset, batch_size, target_size):\n","  '''\n","  Can generate image and mask at the same time use the same seed for image_datagen and mask_datagen to ensure the transformation for image and mask is the same\n","  \n","  datagen: ImageDataGenerator \n","  subset: can take either 'training' or 'validation'\n","  '''\n","  seed = 1\n","  image_generator = image_datagen.flow_from_directory(\n","      os.path.dirname(image_folder_path),\n","      classes = [os.path.basename(image_folder_path)],\n","      class_mode = None,\n","      color_mode = \"grayscale\",\n","      target_size = target_size,\n","      batch_size = batch_size,\n","      subset = subset,\n","      interpolation = \"bicubic\",\n","      seed = seed)\n","  \n","  mask_generator = mask_datagen.flow_from_directory(\n","      os.path.dirname(mask_folder_path),\n","      classes = [os.path.basename(mask_folder_path)],\n","      class_mode = None,\n","      color_mode = \"grayscale\",\n","      target_size = target_size,\n","      batch_size = batch_size,\n","      subset = subset,\n","      interpolation = \"nearest\",\n","      seed = seed)\n","  \n","  this_generator = zip(image_generator, mask_generator)\n","  for (img,mask) in this_generator:\n","      # img,mask = adjustData(img,mask)\n","      yield (img,mask)\n","\n","def prepareGenerators(image_folder_path, mask_folder_path, datagen_parameters, batch_size = 4, target_size = (512, 512)):\n","  image_datagen = ImageDataGenerator(**datagen_parameters, preprocessing_function = normalizePercentile)\n","  mask_datagen = ImageDataGenerator(**datagen_parameters, preprocessing_function = normalizeMinMax)\n","  train_datagen = buildDoubleGenerator(image_datagen, mask_datagen, image_folder_path, mask_folder_path, 'training', batch_size, target_size)\n","  validation_datagen = buildDoubleGenerator(image_datagen, mask_datagen, image_folder_path, mask_folder_path, 'validation', batch_size, target_size)\n","  return (train_datagen, validation_datagen)\n","\n","# Normalization functions from Martin Weigert\n","def normalizePercentile(x, pmin=1, pmax=99.8, axis=None, clip=False, eps=1e-20, dtype=np.float32):\n","    \"\"\"This function is adapted from Martin Weigert\"\"\"\n","    \"\"\"Percentile-based image normalization.\"\"\"\n","\n","    mi = np.percentile(x,pmin,axis=axis,keepdims=True)\n","    ma = np.percentile(x,pmax,axis=axis,keepdims=True)\n","    return normalize_mi_ma(x, mi, ma, clip=clip, eps=eps, dtype=dtype)\n","\n","\n","def normalize_mi_ma(x, mi, ma, clip=False, eps=1e-20, dtype=np.float32):#dtype=np.float32\n","    \"\"\"This function is adapted from Martin Weigert\"\"\"\n","    if dtype is not None:\n","        x   = x.astype(dtype,copy=False)\n","        mi  = dtype(mi) if np.isscalar(mi) else mi.astype(dtype,copy=False)\n","        ma  = dtype(ma) if np.isscalar(ma) else ma.astype(dtype,copy=False)\n","        eps = dtype(eps)\n","\n","    try:\n","        import numexpr\n","        x = numexpr.evaluate(\"(x - mi) / ( ma - mi + eps )\")\n","    except ImportError:\n","        x =                   (x - mi) / ( ma - mi + eps )\n","\n","    if clip:\n","        x = np.clip(x,0,1)\n","\n","    return x\n","\n","# Simple normalization to min/max fir the Mask\n","def normalizeMinMax(x, dtype=np.float32):\n","  x = x.astype(dtype,copy=False)\n","  x = (x - np.amin(x)) / (np.amax(x) - np.amin(x))\n","  return x\n","\n","# This is code outlines the architecture of U-net. The choice of pooling steps decides the depth of the network. \n","def unet(pretrained_weights = None, nchannels=64, input_size = (256,256,1), pooling_steps = 4, learning_rate = 1e-4, verbose=True, class_weights=np.ones(2)):\n","    n = nchannels\n","    inputs = Input(input_size)\n","    conv1 = Conv2D(n, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n","    conv1 = Conv2D(n, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n","    # Downsampling steps\n","    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n","    conv2 = Conv2D(2*n, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n","    conv2 = Conv2D(2*n, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n","    \n","    if pooling_steps > 1:\n","      pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n","      conv3 = Conv2D(4*n, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n","      conv3 = Conv2D(4*n, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n","\n","      if pooling_steps > 2:\n","        pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n","        conv4 = Conv2D(8*n, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n","        conv4 = Conv2D(8*n, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n","        drop4 = Dropout(0.5)(conv4)\n","      \n","        if pooling_steps > 3:\n","          pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n","          conv5 = Conv2D(16*n, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n","          conv5 = Conv2D(16*n, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n","          drop5 = Dropout(0.5)(conv5)\n","          up6 = Conv2D(8*n, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n","          merge6 = concatenate([drop4,up6], axis = 3)\n","          conv6 = Conv2D(8*n, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n","          conv6 = Conv2D(8*n, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n","          \n","    if pooling_steps > 2:\n","      up7 = Conv2D(4*n, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop4))\n","      if pooling_steps > 3:\n","        up7 = Conv2D(4*n, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n","      merge7 = concatenate([conv3,up7], axis = 3)\n","      conv7 = Conv2D(4*n, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n","      conv7 = Conv2D(4*n, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n","\n","    if pooling_steps > 1:\n","      up8 = Conv2D(2*n, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv3))\n","      if pooling_steps > 2:\n","        up8 = Conv2D(2*n, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n","      merge8 = concatenate([conv2,up8], axis = 3)\n","      conv8 = Conv2D(2*n, 3, activation= 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n","      conv8 = Conv2D(2*n, 3, activation= 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n","      \n","    if pooling_steps == 1:\n","      up9 = Conv2D(n, 2, padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv2))\n","    else:\n","      up9 = Conv2D(n, 2, padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8)) #activation = 'relu'\n","    \n","    merge9 = concatenate([conv1,up9], axis = 3)\n","    conv9 = Conv2D(n, 3, padding = 'same', kernel_initializer = 'he_normal')(merge9) #activation = 'relu'\n","    conv9 = Conv2D(n, 3, padding = 'same', kernel_initializer = 'he_normal')(conv9) #activation = 'relu'\n","    conv9 = Conv2D(2, 3, padding = 'same', kernel_initializer = 'he_normal')(conv9) #activation = 'relu'\n","    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n","\n","    model = Model(inputs = inputs, outputs = conv10)\n","\n","    # model.compile(optimizer = Adam(lr = learning_rate), loss = 'binary_crossentropy', metrics = ['acc'])\n","    model.compile(optimizer = Adam(lr = learning_rate), loss = weighted_binary_crossentropy(class_weights))\n","\n","    if(pretrained_weights):\n","    \tmodel.load_weights(pretrained_weights);\n","\n","    return model\n","\n","def predict_as_tiles(Image_path, model):\n","\n","  # Read the data in and normalize\n","  Image_raw = io.imread(Image_path, as_gray = True)\n","  Image_raw = normalizePercentile(Image_raw)\n","\n","  # Get the patch size from the input layer of the model\n","  patch_size = model.input_shape[1:3]\n","\n","  # Pad the image with zeros if any of its dimensions is smaller than the patch size\n","  if Image_raw.shape[0] < patch_size[0] or Image_raw.shape[1] < patch_size[1]:\n","    Image = np.zeros((max(Image_raw.shape[0], patch_size[0]), max(Image_raw.shape[1], patch_size[1])))\n","    Image[0:Image_raw.shape[0], 0: Image_raw.shape[1]] = Image_raw\n","  else:\n","    Image = Image_raw\n","\n","  # Calculate the number of patches in each dimension\n","  n_patch_in_width = ceil(Image.shape[0]/patch_size[0])\n","  n_patch_in_height = ceil(Image.shape[1]/patch_size[1])\n","\n","  prediction = np.zeros(Image.shape)\n","\n","  for x in range(n_patch_in_width):\n","    for y in range(n_patch_in_height):\n","      xi = patch_size[0]*x\n","      yi = patch_size[1]*y\n","\n","      # If the patch exceeds the edge of the image shift it back \n","      if xi+patch_size[0] >= Image.shape[0]:\n","        xi = Image.shape[0]-patch_size[0]\n","\n","      if yi+patch_size[1] >= Image.shape[1]:\n","        yi = Image.shape[1]-patch_size[1]\n","      \n","      # Extract and reshape the patch\n","      patch = Image[xi:xi+patch_size[0], yi:yi+patch_size[1]]\n","      patch = np.reshape(patch,patch.shape+(1,))\n","      patch = np.reshape(patch,(1,)+patch.shape)\n","\n","      # Get the prediction from the patch and paste it in the prediction in the right place\n","      predicted_patch = model.predict(patch, batch_size = 1)\n","      prediction[xi:xi+patch_size[0], yi:yi+patch_size[1]] = np.squeeze(predicted_patch)\n","\n","\n","  return prediction[0:Image_raw.shape[0], 0: Image_raw.shape[1]]\n","  \n","def saveResult(save_path, nparray, source_dir_list, prefix='', threshold=None):\n","  for (filename, image) in zip(source_dir_list, nparray):\n","      io.imsave(os.path.join(save_path, prefix+os.path.splitext(filename)[0]+'.tif'), img_as_ubyte(image)) # saving as unsigned 8-bit image\n","      \n","      # For masks, threshold the images and return 8 bit image\n","      if threshold is not None:\n","        mask = convert2Mask(image, threshold)\n","        io.imsave(os.path.join(save_path, prefix+'mask_'+os.path.splitext(filename)[0]+'.tif'), mask)\n","\n","\n","def convert2Mask(image, threshold):\n","  mask = img_as_ubyte(image, force_copy=True)\n","  mask[mask > threshold] = 255\n","  mask[mask <= threshold] = 0\n","  return mask\n","\n","\n","def getIoUvsThreshold(prediction_filepath, groud_truth_filepath):\n","  prediction = io.imread(prediction_filepath)\n","  ground_truth_image = img_as_ubyte(io.imread(groud_truth_filepath, as_gray=True), force_copy=True)\n","\n","  threshold_list = []\n","  IoU_scores_list = []\n","\n","  for threshold in range(0,256): \n","    # Convert to 8-bit for calculating the IoU\n","    mask = img_as_ubyte(prediction, force_copy=True)\n","    mask[mask > threshold] = 255\n","    mask[mask <= threshold] = 0\n","\n","    # Intersection over Union metric\n","    intersection = np.logical_and(ground_truth_image, np.squeeze(mask))\n","    union = np.logical_or(ground_truth_image, np.squeeze(mask))\n","    iou_score = np.sum(intersection) / np.sum(union)\n","\n","    threshold_list.append(threshold)\n","    IoU_scores_list.append(iou_score)\n","\n","  return (threshold_list, IoU_scores_list)\n","\n","prediction_prefix = 'Predicted_'\n","print()\n","print('The requires function are imported or defined.')\n","\n","# Build requirements file for local run\n","after = [str(m) for m in sys.modules]\n","build_requirements_file(before, after)\n","\n","# Allow TensorBoard\n","#%load_ext tensorboard\n","#!rm -rf ./logs/\n","#log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","#tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n"]},{"cell_type":"markdown","metadata":{"id":"ywZv-2nwPtqu"},"source":["# **2. Google Colab Setting Up**"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"zCvebubeSaGY"},"outputs":[],"source":["#@markdown ### **2.1. Check GPU**\n","if tf.test.gpu_device_name()=='':\n","  print('You do not have GPU access.') \n","  print('Did you change your runtime ?') \n","  print('If the runtime setting is correct then Google did not allocate a GPU for your session')\n","  print('Expect slow performance. To access GPU try reconnecting later')\n","\n","else:\n","  print('You have GPU access')\n","  !nvidia-smi\n","\n","# from tensorflow.python.client import device_lib \n","# device_lib.list_local_devices()\n","\n","# print the tensorflow version\n","print('Tensorflow version is ' + str(tf.__version__))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"01Djr8v-5pPk"},"outputs":[],"source":["#@markdown ### **2.2 Connect to your Google Drive**\n","\n","#@markdown If you cannot see your files, reactivate your session by connecting to your hosted runtime.\n","\n","# mount user's Google Drive to Google Colab.\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3gbAOjalQU1O"},"source":["# **3. Data**"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"7B_EthrsoKMI"},"outputs":[],"source":["#@markdown ### **3.1 Define path to the dataset**\n","#@markdown **`dataset_path`:** should contains 2 folders **training_source** and **training_target** for the training or fine tuning (and optionally 2 other folders for the quality control **test_source** and **test_target**).\n","Dataset_path = '' #@param {type:\"string\"}\n","\n","#@markdown **`percentage_validation`:**  Input the percentage of your training dataset you want to use to validate the network during training. **Default value: 10** \n","percentage_validation =  10 #@param{type:\"number\"}\n","\n","for f in os.listdir(Dataset_path):\n","    if f.lower() == 'training_source' or f.lower() == 'source' or f.lower() == 'train_source':\n","      Training_source = os.path.join(Dataset_path, f)\n","    if f.lower() == 'training_target' or f.lower() == 'target' or f.lower() == 'train_target':\n","      Training_target = os.path.join(Dataset_path, f)\n","    if f.lower() == 'test_source' or f.lower() == 'qc_source':\n","      Source_QC = os.path.join(Dataset_path, f)\n","    if f.lower() == 'test_target' or f.lower() == 'qc_target':\n","      Target_QC = os.path.join(Dataset_path, f)\n","\n","\n","print(os.listdir(Dataset_path))\n","print('Number of files in train_source', len(os.listdir(Training_source)))\n","print('Number of files in train_target', len(os.listdir(Training_target)))\n","print('Number of files in test_source', len(os.listdir(Source_QC)))\n","print('Number of files in test_target', len(os.listdir(Target_QC)))\n","\n","file = os.listdir(Training_source)\n","img = io.imread(os.path.join(Training_source, file[0]))\n","patch_width = img.shape[0]\n","patch_height = img.shape[1]\n","\n","print('Size of image:', patch_width, ' x ', patch_height)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"WS6Dc0ZbTaa7"},"outputs":[],"source":["#@markdown ### **3.2 Read data**\n","\n","#@markdown Read the .tif, .tiff or .png files.\n","\n","def read_data(Training_source, Training_target, min_fraction):\n","  \"\"\"\n","  min_fraction is the minimum fraction of pixels that need to be foreground to be considered as a valid patch\n","  Returns: - Two paths to where the patches are now saved\n","  \"\"\"\n","  Patch_source = os.path.join('/content','img_patches')\n","  Patch_target = os.path.join('/content','mask_patches')\n","  if os.path.exists(Patch_source):\n","    shutil.rmtree(Patch_source)\n","  if os.path.exists(Patch_target):\n","    shutil.rmtree(Patch_target)\n","  os.mkdir(Patch_source)\n","  os.mkdir(Patch_target)\n","  patch_num = 0\n","  table_files = []\n","  table_dim = []\n","  table_image = []\n","  table_mask = []\n","  table_classes = []\n","  table_area = []\n","  table_index = []\n","  index_i = 0\n","  for file in tqdm(os.listdir(Training_source)):\n","    if file.startswith('.'): \n","      print('Rejected ', file)\n","    elif file.endswith('.tif') or file.endswith('.tiff') or file.endswith('.png'):  \n","      img = io.imread(os.path.join(Training_source, file))\n","      patch_width =  img.shape[0]\n","      patch_height =  img.shape[1]\n","      area = patch_width * patch_height\n","      mask = io.imread(os.path.join(Training_target, file),as_gray=True)\n","      area = mask.shape[0] * mask.shape[1]\n","      unique, counts = np.unique(mask, return_counts=True)\n","      index_i = index_i +1;\n","      table_files.append(file)\n","      table_dim.append(str(patch_width)  + \"x\" +str(patch_height))\n","      table_image.append(str(img.shape) + '  [' + str(np.min(img))+ ', ' + str(np.max(img)) + ']')\n","      table_mask.append(str(mask.shape) + '  [' + str(np.min(mask))+ ', ' + str(np.max(mask)) + ']')\n","      table_area.append(str(unique) + ' ' + str(np.round(100*counts/area, 2)))\n","      table_index.append(index_i)\n","      result = np.column_stack((unique, counts)) \n","      patches_img = view_as_windows(img, (patch_width, patch_height), (patch_width, patch_height))\n","      patches_mask = view_as_windows(mask, (patch_width, patch_height), (patch_width, patch_height))\n","      patches_img = patches_img.reshape(patches_img.shape[0]*patches_img.shape[1], patch_width,patch_height)\n","      patches_mask = patches_mask.reshape(patches_mask.shape[0]*patches_mask.shape[1], patch_width,patch_height)\n"," \n","      for i in range(patches_img.shape[0]):\n","        img_save_path = os.path.join(Patch_source,'image_'+str(patch_num)+'.tif')\n","        mask_save_path = os.path.join(Patch_target,'image_'+str(patch_num)+'.tif')\n","        patch_num += 1\n","        pixel_threshold_array = sorted(patches_mask[i].flatten())\n","        if pixel_threshold_array[int(round(len(pixel_threshold_array)*(1-min_fraction)))]>0:\n","          io.imsave(img_save_path, img_as_ubyte(normalizeMinMax(patches_img[i])))\n","          io.imsave(mask_save_path, convert2Mask(normalizeMinMax(patches_mask[i]),0))\n","        else:\n","          print('Rejected min-frac ', file, ' less than ', min_fraction)\n","  list_of_files = pd.DataFrame({\n","          'Filename': table_files,\n","          'Source (dim, type, min, max)': table_image,\n","          'Target (dim, type, min, max)': table_mask,\n","          'Target (classes / area)': table_area\n","          }, index = table_index)\n","\n","  return Patch_source, Patch_target, list_of_files\n","\n","\n","# The minimum fraction of pixels being foreground for a selected patch to be considered valid is set to the default value: 2%.\n","min_fraction = 0.02\n","Patch_source, Patch_target, list_of_files = read_data(Training_source, Training_target, min_fraction)\n","\n","data_table.DataTable(list_of_files, num_rows_per_page=10)\n","\n","number_of_training_dataset = len(os.listdir(Patch_source))\n","print('Total number of valid images: '+str(number_of_training_dataset))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"gM9iz-vbgyY7"},"outputs":[],"source":["#@markdown ### **3.3 List training images (optional)**\n","\n","data_table.DataTable(list_of_files, num_rows_per_page=25)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"TZKdRJ11Qq-u"},"outputs":[],"source":["#@markdown ### **3.4 Display source image and target (optional)**\n","\n","#random_choice = random.choice(os.listdir(Patch_source))\n","#x = io.imread(os.path.join(Patch_source, random_choice))\n","#y = io.imread(os.path.join(Patch_target, random_choice), as_gray=True)\n","#f=plt.figure(figsize=(16,8))\n","#plt.subplot(1,2,1)\n","#plt.imshow(x, interpolation='nearest',cmap='gray')\n","#plt.title('Training image patch')\n","#plt.axis('off');\n","#plt.subplot(1,2,2)\n","#plt.imshow(y, interpolation='nearest',cmap='gray')\n","#plt.title('Training mask patch')\n","#plt.axis('off');\n","#plt.savefig('/content/TrainingDataExample_Unet2D.png',bbox_inches='tight',pad_inches=0)\n","\n","data_table.DataTable(list_of_files, num_rows_per_page=25)\n","\n","\n","# ------------- For display ------------\n","@interact\n","def show_QC_results(file=os.listdir(Patch_source)):\n","  plt.figure(figsize=(16,8))\n","  #Input\n","  plt.subplot(1,2,1)\n","  plt.axis('off')\n","  plt.imshow(plt.imread(os.path.join(Patch_source, file)), aspect='equal', cmap='gray', interpolation='nearest')\n","  plt.title('Source')\n","  plt.subplot(1,2,2)\n","  plt.axis('off')\n","  test_ground_truth_image = io.imread(os.path.join(Patch_target, file),as_gray=True)\n","  plt.imshow(test_ground_truth_image, aspect='equal', cmap='gray')\n","  plt.title('Target')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"1k-v7ElSZwqz"},"outputs":[],"source":["#@markdown ### **3.5 Data Augmentation (Recommended)**\n","\n","Use_Data_augmentation = True #@param {type:\"boolean\"}\n","\n","if Use_Data_augmentation:\n","    horizontal_shift =  6 #@param {type:\"slider\", min:0, max:100, step:1}\n","    vertical_shift =  10 #@param {type:\"slider\", min:0, max:100, step:1}\n","    zoom_range =  10 #@param {type:\"slider\", min:0, max:100, step:1}\n","    shear_range =  10 #@param {type:\"slider\", min:0, max:100, step:1}\n","    horizontal_flip = True #@param {type:\"boolean\"}\n","    vertical_flip = True #@param {type:\"boolean\"}\n","    rotation_range =  180 #@param {type:\"slider\", min:0, max:180, step:1}\n","else:\n","  horizontal_shift =  0 \n","  vertical_shift =  0 \n","  zoom_range =  0\n","  shear_range =  0\n","  horizontal_flip = False\n","  vertical_flip = False\n","  rotation_range =  0\n","\n","# Build the dict for the ImageDataGenerator\n","data_gen_args = dict(width_shift_range = horizontal_shift/100.,\n","                     height_shift_range = vertical_shift/100.,\n","                     rotation_range = rotation_range, #90\n","                     zoom_range = zoom_range/100.,\n","                     shear_range = shear_range/100.,\n","                     horizontal_flip = horizontal_flip,\n","                     vertical_flip = vertical_flip,\n","                     validation_split = percentage_validation/100,\n","                     fill_mode = 'reflect')\n","\n","# ------------- Display ------------\n","dir_augmented_data_imgs=\"/content/augment_img\"\n","dir_augmented_data_masks=\"/content/augment_mask\"\n","random_choice = random.choice(os.listdir(Patch_source))\n","orig_img = load_img(os.path.join(Patch_source,random_choice))\n","orig_mask = load_img(os.path.join(Patch_target,random_choice))\n","\n","augment_view = ImageDataGenerator(**data_gen_args)\n","\n","if Use_Data_augmentation:\n","  print(\"Parameters enabled\")\n","  print(\"Here is what a subset of your augmentations looks like:\")\n","  save_augment(augment_view, orig_img, dir_augmented_data=dir_augmented_data_imgs)\n","  save_augment(augment_view, orig_mask, dir_augmented_data=dir_augmented_data_masks)\n","\n","  fig = plt.figure(figsize=(15, 7))\n","  fig.subplots_adjust(hspace=0.0,wspace=0.1,left=0,right=1.1,bottom=0, top=0.8)\n","\n","  ax = fig.add_subplot(2, 6, 1,xticks=[],yticks=[])        \n","  new_img=img_as_ubyte(normalizeMinMax(img_to_array(orig_img)))\n","  ax.imshow(new_img)\n","  ax.set_title('Original Image')\n","  i = 2\n","  for imgnm in os.listdir(dir_augmented_data_imgs):\n","    ax = fig.add_subplot(2, 6, i,xticks=[],yticks=[]) \n","    img = load_img(dir_augmented_data_imgs + \"/\" + imgnm)\n","    ax.imshow(img)\n","    i += 1\n","\n","  ax = fig.add_subplot(2, 6, 7,xticks=[],yticks=[])        \n","  new_mask=img_as_ubyte(normalizeMinMax(img_to_array(orig_mask)))\n","  ax.imshow(new_mask)\n","  ax.set_title('Original Mask')\n","  j=2\n","  for imgnm in os.listdir(dir_augmented_data_masks):\n","    ax = fig.add_subplot(2, 6, j+6,xticks=[],yticks=[]) \n","    mask = load_img(dir_augmented_data_masks + \"/\" + imgnm)\n","    ax.imshow(mask)\n","    j += 1\n","  plt.show()\n","\n","else:\n","  print(\"No augmentation will be used\")"]},{"cell_type":"markdown","metadata":{"id":"7YJegsdHt2S2"},"source":["# **4. U-Net Configuration**"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"1yPWKZsKKULm"},"outputs":[],"source":["#@markdown ### **4.1 Define name and hyper-parameters**\n","\n","model_path = \"\" #@param {type:\"string\"}\n","model_name = '' #@param {type:\"string\"}\n","number_of_epochs =  5#@param {type:\"number\"}\n","\n","Use_pretrained_model = False\n","#@markdown **Advanced parameters - experienced users only**\n","\n","#@markdown **`nchannels`** This is the number of filters of the UNet at the first scale **Default value: 64**\n","nchannels = 64 #@param {type:\"integer\"}\n","#@markdown **`batch_size`**: This parameter describes the amount of images that are loaded into the network per step. Smaller batchsizes may improve training performance slightly but may increase training time.  **Default: 4**\n","batch_size =  4 #@param {type:\"integer\"}\n","number_of_steps =  0\n","#@markdown **`pooling_steps`**: Each additional pooling step will also two additional convolutions. The network can learn more complex information but is also more likely to overfit. Achieving best performance may require testing different values here. **Default: 2**\n","pooling_steps = 3 #@param [1,2,3,4]{type:\"raw\"}\n","#@markdown **`initial_learning_rate`:**  Input the initial value to be used as learning rate. **Default value: 0.0003**\n","initial_learning_rate = 0.0003 #@param {type:\"number\"}\n","\n","#  Create the folders where to save the model and the QC\n","full_model_path = os.path.join(model_path, model_name)\n","\n","print('Model path: ', full_model_path)\n","# Here we disable pre-trained model by default (in case the next cell is not ran)\n","Use_pretrained_model = False\n","# Here we disable data augmentation by default (in case the cell is not ran)\n","# Use_Data_augmentation = False\n","# Build the default dict for the ImageDataGenerator\n","# data_gen_args = dict(width_shift_range = 0.,\n","#                      height_shift_range = 0.,\n","#                      rotation_range = 0., #90\n","#                      zoom_range = 0.,\n","#                      shear_range = 0.,\n","#                      horizontal_flip = False,\n","#                      vertical_flip = False,\n","#                      validation_split = percentage_validation/100,\n","#                      fill_mode = 'reflect')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"9vC2n-HeLdiJ"},"outputs":[],"source":["#@markdown ### **4.2 Fine-tuning (optional)**\n","\n","#@markdown Loading weights from a pre-trained network. This is not required to train a network from scratch\n","\n","Use_pretrained_model = False #@param {type:\"boolean\"}\n","pretrained_model_choice = \"Model_from_file\" #@param [\"Model_from_file\", \"bioimageio_model\"]\n","Weights_choice = \"best\" #@param [\"last\", \"best\"]\n","\n","#@markdown If you chose \"Model_from_file\", please provide the path to the model folder and the model id:\n","pretrained_model_path = \"\" #@param {type:\"string\"}\n","bioimageio_model_id = \"\" #@param {type:\"string\"}\n","\n","# --------------------- Check if we load a previously trained model ------------------------\n","if Use_pretrained_model:\n","\n","# --------------------- Load the model from the choosen path ------------------------\n","  if pretrained_model_choice == \"Model_from_file\":\n","    h5_file_path = os.path.join(pretrained_model_path, \"weights_\"+Weights_choice+\".hdf5\")\n","    qc_path = os.path.join(pretrained_model_path, 'Quality Control', 'training_evaluation.csv')\n","\n","# --------------------- Load the model from a bioimageio model (can be path on drive or url / doi) ---\n","  elif pretrained_model_choice == \"bioimageio_model\":\n","    biomodel = load_resource_description(bioimageio_model_id)  \n","    if \"keras_hdf5\" not in biomodel.weights:\n","      print(\"Invalid bioimageio model\")\n","      h5_file_path = \"no-model\"\n","      qc_path = \"no-qc\"\n","    else:\n","      h5_file_path = \"/content/bioimage_model_zoo/\"\n","      %rm -rf $h5_file_path\n","      \n","      h5_file_path = str(biomodel.weights[\"keras_hdf5\"].source)\n","      try:\n","        attachments = biomodel.attachments.files\n","        qc_path = [fname for fname in attachments if fname.endswith(\"training_evaluation.csv\")][0]\n","        qc_path = os.path.join(\"/content/bioimageio_pretrained_model\", qc_path)\n","      except Exception:\n","        qc_path = \"no-qc\"\n","# --------------------- Check the model exist ------------------------\n","# If the model path chosen does not contain a pretrain model then use_pretrained_model is disabled, \n","  if not os.path.exists(h5_file_path):\n","    print(R+'WARNING: pretrained model does not exist')\n","    Use_pretrained_model = False\n","    \n","# If the model path contains a pretrain model, we load the training rate, \n","  if os.path.exists(h5_file_path):\n","#Here we check if the learning rate can be loaded from the quality control folder\n","    if os.path.exists(qc_path):\n","\n","      with open(qc_path,'r') as csvfile:\n","        csvRead = pd.read_csv(csvfile, sep=',')\n","        #print(csvRead)\n","    \n","        if \"learning rate\" in csvRead.columns: #Here we check that the learning rate column exist (compatibility with model trained un ZeroCostDL4Mic bellow 1.4)\n","          print(\"pretrained network learning rate found\")\n","          #find the last learning rate\n","          lastLearningRate = csvRead[\"learning rate\"].iloc[-1]\n","          #Find the learning rate corresponding to the lowest validation loss\n","          min_val_loss = csvRead[csvRead['val_loss'] == min(csvRead['val_loss'])]\n","          #print(min_val_loss)\n","          bestLearningRate = min_val_loss['learning rate'].iloc[-1]\n","\n","          if Weights_choice == \"last\":\n","            print('Last learning rate: '+str(lastLearningRate))\n","\n","          if Weights_choice == \"best\":\n","            print('Learning rate of best validation loss: '+str(bestLearningRate))\n","\n","        if not \"learning rate\" in csvRead.columns: #if the column does not exist, then initial learning rate is used instead\n","          bestLearningRate = initial_learning_rate\n","          lastLearningRate = initial_learning_rate\n","          print(bcolors.WARNING+'WARNING: The learning rate cannot be identified from the pretrained network. Default learning rate of '+str(bestLearningRate)+' will be used instead' + W)\n","\n","#Compatibility with models trained outside ZeroCostDL4Mic but default learning rate will be used\n","    if not os.path.exists(os.path.join(pretrained_model_path, 'Quality Control', 'training_evaluation.csv')):\n","      print(bcolors.WARNING+'WARNING: The learning rate cannot be identified from the pretrained network. Default learning rate of '+str(initial_learning_rate)+' will be used instead'+ W)\n","      bestLearningRate = initial_learning_rate\n","      lastLearningRate = initial_learning_rate\n","\n","\n","# Display info about the pretrained model to be loaded (or not)\n","if Use_pretrained_model:\n","  print('Weights found in:')\n","  print(h5_file_path)\n","  print('will be loaded prior to training.')\n","else:\n","  print('No pretrained network will be used.')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"PwzzuaHuuk-Q"},"outputs":[],"source":["#@markdown ### **4.3 Prepare network**\n","#@markdown Creation of the network and configuration of the optimizer\n","warnings.filterwarnings('ignore')\n","#print('Training on patches of size (x,y): ('+str(patch_width)+','+str(patch_height)+')')\n","\n","if number_of_steps == 0:\n","  number_of_steps = ceil((100-percentage_validation)/100*number_of_training_dataset/batch_size)\n","#print('Number of steps: '+str(number_of_steps))\n","(train_datagen, validation_datagen) = prepareGenerators(Patch_source, Patch_target, data_gen_args, batch_size, target_size = (patch_width, patch_height))\n","# Calculate the number of steps to use for validation\n","validation_steps = max(1, ceil(percentage_validation/100*number_of_training_dataset/batch_size))\n","\n","# This modelcheckpoint will only save the best model from the validation loss point of view\n","model_checkpoint = ModelCheckpoint(os.path.join(full_model_path, 'weights_best.hdf5'), monitor='val_loss',verbose=1, save_best_only=True)\n","\n","#print('Getting class weights...')\n","class_weights = getClassWeights(Training_target)\n","\n","#Print_network_model = True #@param {type:\"boolean\"}\n","#Draw_network_model = True #@param {type:\"boolean\"}\n","#Save_PDF_network_model = True #@param {type:\"boolean\"}\n","\n","# --------------------- Using pretrained model ------------------------\n","#Here we ensure that the learning rate set correctly when using pre-trained models\n","if Use_pretrained_model:\n","  if Weights_choice == \"last\":\n","    initial_learning_rate = lastLearningRate\n","\n","  if Weights_choice == \"best\":            \n","    initial_learning_rate = bestLearningRate\n","else:\n","  h5_file_path = None\n","\n","# Reduce learning rate on plateau\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, verbose=1, mode='auto', patience=10, min_lr=0)\n","\n","# Define the model\n","model = unet(input_size = (patch_width,patch_height,1), \n","             nchannels = nchannels, pooling_steps = pooling_steps, \n","             learning_rate = initial_learning_rate, class_weights = class_weights)\n","\n","# --------------------- Using pretrained model ------------------------\n","# Load the pretrained weights \n","if Use_pretrained_model:\n","  try:\n","      model.load_weights(h5_file_path)\n","  except:\n","      print(\"The pretrained model could not be loaded as the configuration of the network is different.\")\n"," \n","config_model= model.optimizer.get_config()\n","\n","print()\n","print(\"Optimizer configuration : \" )\n","print(config_model)\n","print()\n","\n","# ------------------ Failsafes ------------------\n","if os.path.exists(full_model_path):\n","  print('!! WARNING: Model folder already existed and has been removed !!')\n","  print()\n","  shutil.rmtree(full_model_path)\n","\n","os.makedirs(full_model_path)\n","os.makedirs(os.path.join(full_model_path,'Quality Control'))\n","\n","# ------------------ Display ------------------\n","print('---------------------------- Main training parameters ----------------------------')\n","print('Number of epochs: '+str(number_of_epochs))\n","print('Batch size: '+str(batch_size))\n","print('Number of training dataset: '+str(number_of_training_dataset))\n","print('Number of training steps: '+str(number_of_steps))\n","print('Number of validation steps: '+str(validation_steps))\n","print('---------------------------- ------------------------ ----------------------------')\n","\n","def pdf_export(trained = False, augmentation = False, pretrained_model = False):\n","  class MyFPDF(FPDF, HTMLMixin):\n","    pass\n","\n","  pdf = MyFPDF()\n","  pdf.add_page()\n","  pdf.set_right_margin(-1)\n","  pdf.set_font(\"Arial\", size = 11, style='B') \n","\n","  day = datetime.now()\n","  datetime_str = str(day)[0:10]\n","\n","  Header = 'Training report for '+Network+' model ('+model_name+')\\nDate: '+datetime_str\n","  pdf.multi_cell(180, 5, txt = Header, align = 'L') \n","    \n","  # add another cell \n","  if trained:\n","    training_time = \"Training time: \"+str(hour)+ \"hour(s) \"+str(mins)+\"min(s) \"+str(round(sec))+\"sec(s)\"\n","    pdf.cell(190, 5, txt = training_time, ln = 1, align='L')\n","  pdf.ln(1)\n","\n","  Header_2 = 'Information for your materials and method:'\n","  pdf.cell(190, 5, txt=Header_2, ln=1, align='L')\n","\n","  all_packages = ''\n","  for requirement in freeze(local_only=True):\n","    all_packages = all_packages+requirement+', '\n","  #print(all_packages)\n","\n","  #Main Packages\n","  main_packages = ''\n","  version_numbers = []\n","  for name in ['tensorflow','numpy','Keras']:\n","    find_name=all_packages.find(name)\n","    main_packages = main_packages+all_packages[find_name:all_packages.find(',',find_name)]+', '\n","    #Version numbers only here:\n","    version_numbers.append(all_packages[find_name+len(name)+2:all_packages.find(',',find_name)])\n","\n","  cuda_version = subprocess.run('nvcc --version',stdout=subprocess.PIPE, shell=True)\n","  cuda_version = cuda_version.stdout.decode('utf-8')\n","  cuda_version = cuda_version[cuda_version.find(', V')+3:-1]\n","  gpu_name = subprocess.run('nvidia-smi',stdout=subprocess.PIPE, shell=True)\n","  gpu_name = gpu_name.stdout.decode('utf-8')\n","  gpu_name = gpu_name[gpu_name.find('Tesla'):gpu_name.find('Tesla')+10]\n","  #print(cuda_version[cuda_version.find(', V')+3:-1])\n","  #print(gpu_name)\n","  loss = str(model.loss)[str(model.loss).find('function')+len('function'):str(model.loss).find('.<')]\n","  shape = io.imread(Training_source+'/'+os.listdir(Training_source)[1]).shape\n","  dataset_size = len(os.listdir(Training_source))\n","\n","  text = 'The '+Network+' model was trained from scratch for '+str(number_of_epochs)+' epochs on '+str(number_of_training_dataset)+' paired image patches (image dimensions: '+str(shape)+', patch size: ('+str(patch_width)+','+str(patch_height)+')) with a batch size of '+str(batch_size)+' and a'+loss+' loss function,'+' using the '+Network+' ZeroCostDL4Mic notebook (v '+Notebook_version[0]+') (von Chamier & Laine et al., 2020). Key python packages used include tensorflow (v '+version_numbers[0]+'), Keras (v '+version_numbers[2]+'), numpy (v '+version_numbers[1]+'), cuda (v '+cuda_version+'). The training was accelerated using a '+gpu_name+'GPU.'\n","\n","  if pretrained_model:\n","    text = 'The '+Network+' model was trained for '+str(number_of_epochs)+' epochs on '+str(number_of_training_dataset)+' paired image patches (image dimensions: '+str(shape)+', patch size: ('+str(patch_width)+','+str(patch_height)+')) with a batch size of '+str(batch_size)+'  and a'+loss+' loss function,'+' using the '+Network+' ZeroCostDL4Mic notebook (v '+Notebook_version[0]+') (von Chamier & Laine et al., 2020). The model was re-trained from a pretrained model. Key python packages used include tensorflow (v '+version_numbers[0]+'), Keras (v '+version_numbers[2]+'), numpy (v '+version_numbers[1]+'), cuda (v '+cuda_version+'). The training was accelerated using a '+gpu_name+'GPU.'\n","\n","  pdf.set_font('')\n","  pdf.set_font_size(10.)\n","  pdf.multi_cell(180, 5, txt = text, align='L')\n","  pdf.set_font('')\n","  pdf.set_font('Arial', size = 10, style = 'B')\n","  pdf.ln(1)\n","  pdf.cell(28, 5, txt='Augmentation: ', ln=1)\n","  pdf.set_font('')\n","  if augmentation:\n","    aug_text = 'The dataset was augmented by'\n","    if rotation_range != 0:\n","      aug_text = aug_text+'\\n- rotation'\n","    if horizontal_flip == True or vertical_flip == True:\n","      aug_text = aug_text+'\\n- flipping'\n","    if zoom_range != 0:\n","      aug_text = aug_text+'\\n- random zoom magnification'\n","    if horizontal_shift != 0 or vertical_shift != 0:\n","      aug_text = aug_text+'\\n- shifting'\n","    if shear_range != 0:\n","      aug_text = aug_text+'\\n- image shearing'\n","  else:\n","    aug_text = 'No augmentation was used for training.'\n","  pdf.multi_cell(190, 5, txt=aug_text, align='L')\n","  pdf.set_font('Arial', size = 11, style = 'B')\n","  pdf.ln(1)\n","  pdf.cell(180, 5, txt = 'Parameters', align='L', ln=1)\n","  pdf.set_font('')\n","  pdf.set_font_size(10.)\n","  pdf.cell(200, 5, txt='The following parameters were used for training:')\n","  pdf.ln(1)\n","  html = \"\"\" \n","  <table width=40% style=\"margin-left:0px;\">\n","    <tr>\n","      <th width = 50% align=\"left\">Parameter</th>\n","      <th width = 50% align=\"left\">Value</th>\n","    </tr>\n","    <tr>\n","      <td width = 50%>number_of_epochs</td>\n","      <td width = 50%>{0}</td>\n","    </tr>\n","    <tr>\n","      <td width = 50%>image_size</td>\n","      <td width = 50%>{1}</td>\n","    </tr>\n","    <tr>\n","      <td width = 50%>batch_size</td>\n","      <td width = 50%>{2}</td>\n","    </tr>\n","    <tr>\n","      <td width = 50%>number_of_steps</td>\n","      <td width = 50%>{3}</td>\n","    </tr>\n","    <tr>\n","      <td width = 50%>percentage_validation</td>\n","      <td width = 50%>{4}</td>\n","    </tr>\n","    <tr>\n","      <td width = 50%>initial_learning_rate</td>\n","      <td width = 50%>{5}</td>\n","    </tr>\n","    <tr>\n","      <td width = 50%>pooling_steps</td>\n","      <td width = 50%>{6}</td>\n","    </tr>\n","    <tr>\n","      <td width = 50%>nchannels</td>\n","      <td width = 50%>{7}</td>\n","  </table>\n","  \"\"\".format(number_of_epochs, str(patch_width)+'x'+str(patch_height), batch_size, number_of_steps, percentage_validation, initial_learning_rate, pooling_steps, \n","nchannels)\n","  pdf.write_html(html)\n","\n","  #pdf.multi_cell(190, 5, txt = text_2, align='L')\n","  pdf.set_font(\"Arial\", size = 11, style='B')\n","  pdf.ln(1)\n","  pdf.cell(190, 5, txt = 'Training Dataset', align='L', ln=1)\n","  pdf.set_font('')\n","  pdf.set_font('Arial', size = 10, style = 'B')\n","  pdf.cell(29, 5, txt= 'Training_source:', align = 'L', ln=0)\n","  pdf.set_font('')\n","  pdf.multi_cell(170, 5, txt = Training_source, align = 'L')\n","  pdf.set_font('')\n","  pdf.set_font('Arial', size = 10, style = 'B')\n","  pdf.cell(28, 5, txt= 'Training_target:', align = 'L', ln=0)\n","  pdf.set_font('')\n","  pdf.multi_cell(170, 5, txt = Training_target, align = 'L')\n","  #pdf.cell(190, 5, txt=aug_text, align='L', ln=1)\n","  pdf.ln(1)\n","  pdf.set_font('')\n","  pdf.set_font('Arial', size = 10, style = 'B')\n","  pdf.cell(21, 5, txt= 'Model Path:', align = 'L', ln=0)\n","  pdf.set_font('')\n","  pdf.multi_cell(170, 5, txt = model_path+'/'+model_name, align = 'L')\n","  pdf.ln(1)\n","  pdf.cell(60, 5, txt = 'Example Training pair', ln=1)\n","  pdf.ln(1)\n","  #exp_size = io.imread('/content/TrainingDataExample_Unet2D.png').shape\n","  #pdf.image('/content/TrainingDataExample_Unet2D.png', x = 11, y = None, w = round(exp_size[1]/8), h = round(exp_size[0]/8))\n","  pdf.ln(1)\n","  ref_1 = 'References:\\n - ZeroCostDL4Mic: von Chamier, Lucas & Laine, Romain, et al. \"Democratising deep learning for microscopy with ZeroCostDL4Mic.\" Nature Communications (2021).'\n","  pdf.multi_cell(190, 5, txt = ref_1, align='L')\n","  ref_2 = '- Unet: Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. \"U-net: Convolutional networks for biomedical image segmentation.\" International Conference on Medical image computing and computer-assisted intervention. Springer, Cham, 2015.'\n","  pdf.multi_cell(190, 5, txt = ref_2, align='L')\n","  # if Use_Data_augmentation:\n","  #   ref_3 = '- Augmentor: Bloice, Marcus D., Christof Stocker, and Andreas Holzinger. \"Augmentor: an image augmentation library for machine learning.\" arXiv preprint arXiv:1708.04680 (2017).'\n","  #   pdf.multi_cell(190, 5, txt = ref_3, align='L')\n","  pdf.ln(3)\n","  reminder = 'Important:\\nRemember to perform the quality control step on all newly trained models\\nPlease consider depositing your training dataset on Zenodo'\n","  pdf.set_font('Arial', size = 11, style='B')\n","  pdf.multi_cell(190, 5, txt=reminder, align='C')\n","\n","  pdf.output(model_path+'/'+model_name+'/'+model_name+'_model_configuration.pdf')\n","  print('PDF report exported in '+model_path+'/'+model_name+'/')\n","\n","#if Print_network_model:\n","model.summary()\n","\n","#if Draw_network_model:\n","#i  tf.keras.utils.plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n","\n","\n","#if Save_PDF_network_model:\n"," # pdf_export(augmentation = Use_Data_augmentation, pretrained_model = Use_pretrained_model)\n"]},{"cell_type":"markdown","metadata":{"id":"wQzIBa5BRGuX"},"source":["# **5. Training**"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"iwNmp1PUzRDQ","scrolled":true},"outputs":[],"source":["#@markdown ### **5.1 Start training**\n","\n","#log_dir = \"logs/fit/log\"\n","#tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n","start = time.time()\n","tracker = EmissionsTracker(log_level='error')\n","tracker.start()\n","\n","history = model.fit_generator(train_datagen, steps_per_epoch = number_of_steps, \\\n","  epochs = number_of_epochs, \n","  callbacks=[model_checkpoint, reduce_lr], \\\n","  validation_data = validation_datagen, \n","  validation_steps = validation_steps, \n","  shuffle=True, verbose=1)\n","\n","emissions: float = tracker.stop()\n","print()\n","print(\"------------------------------------------\")\n","print(f\"[CodeCarbon] Emissions: {emissions} kg\")\n","\n","model.save(os.path.join(full_model_path, 'weights_last.hdf5'))\n","\n","# convert the history.history dict to a pandas DataFrame:     \n","lossData = pd.DataFrame(history.history) \n","#print(lossData)\n","#data_table.DataTable(lossData, num_rows_per_page=25)\n","\n","# The training evaluation.csv is saved (overwrites the Files if needed). \n","lossDataCSVpath = os.path.join(full_model_path,'Quality Control/training_evaluation.csv')\n","with open(lossDataCSVpath, 'w') as f:\n","  writer = csv.writer(f)\n","  writer.writerow(['loss','val_loss', 'learning rate'])\n","  for i in range(len(history.history['loss'])):\n","    writer.writerow([history.history['loss'][i], history.history['val_loss'][i], history.history['lr'][i]])\n","\n","# Displaying the time elapsed for training\n","dt = time.time() - start\n","mins, sec = divmod(dt, 60) \n","hour, mins = divmod(mins, 60) \n","print(\"------------------------------------------\")\n","#print(\"Time elapsed:\", dt)\n","print(\"Training time:\", hour, \"hour(s)\", mins,\"min(s)\",round(sec),\"sec(s)\")\n","\n","print(\"------------------------------------------\")\n","print('Last loss: ', history.history['loss'][-1],'Last val_loss:', history.history['val_loss'][-1])\n","print('Best loss: ', min(history.history['loss']),'Best val_loss:', min(history.history['val_loss']))\n","\n","print(\"------------------------------------------\")\n","pdf_export(trained = True, augmentation = Use_Data_augmentation, pretrained_model = Use_pretrained_model)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"74pUQuMzPYal"},"outputs":[],"source":["#@markdown ### **5.2 Plot the learning curves**\n","model_weights = 'weights_best.hdf5'\n","model_save = os.path.join(model_path, model_name)\n","model_file = os.path.join(model_save, model_weights)\n","\n","#print('model_save:', model_save)\n","#print('model_file:', model_file)\n","if os.path.exists(model_file):\n","  print(\"The \"+model_weights+\" network is evaluated\")\n","else:\n","  print(R+'!! WARNING: The chosen model '+model_file+'does not exist !!'+W)\n","  print('Please make sure you provide a valid model path and model name before proceeding further.')\n","\n","epochNumber = []\n","lossDataFromCSV = []\n","vallossDataFromCSV = []\n","\n","with open(os.path.join(model_save, 'Quality Control', 'training_evaluation.csv'),'r') as csvfile:\n","    csvRead = csv.reader(csvfile, delimiter=',')\n","    next(csvRead)\n","    for row in csvRead:\n","        lossDataFromCSV.append(float(row[0]))\n","        vallossDataFromCSV.append(float(row[1]))\n","\n","epochNumber = range(len(lossDataFromCSV))\n","\n","plt.figure(figsize=(15,10))\n","\n","plt.subplot(2,1,1)\n","plt.plot(epochNumber,lossDataFromCSV, label='Training loss')\n","plt.plot(epochNumber,vallossDataFromCSV, label='Validation loss')\n","plt.title('Training loss and validation loss vs. epoch number (linear scale)')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch number')\n","plt.legend()\n","\n","plt.subplot(2,1,2)\n","plt.semilogy(epochNumber,lossDataFromCSV, label='Training loss')\n","plt.semilogy(epochNumber,vallossDataFromCSV, label='Validation loss')\n","plt.title('Training loss and validation loss vs. epoch number (log scale)')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch number')\n","plt.legend()\n","plt.savefig(os.path.join(model_save, 'Quality Control', 'lossCurvePlots.png'),bbox_inches='tight',pad_inches=0)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"s-rFxkpeRQ8k"},"source":["# **6. Evaluate the trained model**"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"-gXJ6R58VhG1"},"outputs":[],"source":["\n","#@markdown  ### **6.1 Prediction on the test dataset**\n","\n","#@markdown This section generates the probability maps of all images provided in the test_source folder\n","\n","# #@markdown **`Data_folder`:** This folder should contain the images that you want to use your trained network on for processing.\n","\n","# #@markdown **`Probability_folder`:** This folder will contain the probabity map (float32) as image.\n","\n","#Data_folder = '/content/gdrive/MyDrive/test Goteborg/ctc-DIC-C2DH-HeLa/test_source' #@param {type:\"string\"}\n","#Probability_folder = '/content/gdrive/MyDrive/test Goteborg/Results' #@param {type:\"string\"}\n","\n","warnings.filterwarnings('ignore')\n","\n","Data_folder = Source_QC\n","Probability_folder = os.path.join(model_save,'Prediction')\n","if os.path.exists(Probability_folder):\n","  shutil.rmtree(Probability_folder)\n","os.makedirs(Probability_folder)\n","\n","\n","full_Probability_model_path = os.path.join(model_save, model_weights)\n","if os.path.exists(full_Probability_model_path):\n","  print(\"The model \"+full_Probability_model_path+\" will be used.\")\n","else:\n","  print(R+'!! WARNING: The chosen model does not exist !!'+W)\n","  print('Please make sure you provide a valid model path and model name before proceeding further.')\n","\n","print()\n","# Load the model and prepare generator\n","model_save = full_model_path\n","model_weights =  \"weights_best.hdf5\"\n","#print('The model ' + os.path.join(model_save, model_weights) + ' will be used')\n","\n","unet = load_model(os.path.join(model_save, model_weights), custom_objects={'_weighted_binary_crossentropy': weighted_binary_crossentropy(np.ones(2))})\n","Input_size = unet.layers[0].output_shape[0]\n","#print('Model input size: '+str(Input_size[0])+'x'+str(Input_size[1]))\n","\n","# Create a list of sources\n","source_dir_list = os.listdir(Data_folder)\n","number_of_dataset = len(source_dir_list)\n","print('Number of dataset found in the folder: '+str(number_of_dataset))\n","\n","for i in tqdm(range(number_of_dataset)):\n","  root_filename = os.path.splitext(source_dir_list[i])\n","  prediction = predict_as_tiles(os.path.join(Data_folder, source_dir_list[i]), unet)\n","  # predictions.append(prediction(os.path.join(Data_folder, source_dir_list[i]), os.path.join(Prediction_model_path, Prediction_model_name)))\n","  io.imsave(os.path.join(Probability_folder, 'proba_'+root_filename[0]+'.tif'), img_as_float32(prediction)) # saving as unsigned 8-bit image\n","      \n"]},{"cell_type":"markdown","metadata":{"id":"DEp9VM4sH5hh"},"source":["---\n","\n","# Exercise session\n","\n","1.   Do a new training with a different number of epochs (between 10 and 50)\n","*ATTENTION: You only need to recompute cell 1.2 and the cells from 4.1 to 6.1 with the good parameters*\n","\n","2.   With ImageJ, compute the **IoU** score for **3** predictions:\n","    *   Do a **threshold** to obtain a binary mask\n","    *   Perform a **watershed** to well separate two instances of cells\n","    *   Compute the **IoU** score:\n","$$\\frac{Target \\ \\cap \\ Source}{Target \\ \\cup \\ Prediction}$$\n","\n","    *   Save the results on the excel file that is on the google drive of the course\n","\n","3.   To better visualize the quality of the obtained segmentation, with ImageJ, do a composite image to superimpose the binary masks on the corresponding test images.\n","\n","4.   If you have time, you can do it for different number of training epochs\n","\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d1wAZtnzMMsg","cellView":"form"},"outputs":[],"source":["# ------------- User input ------------\n","#@markdown ### **6.2 Quality Control** \n","print('The model ' + os.path.join(model_save, model_weights) + ' will be used')\n","#@markdown This section calculates the **IoU** score for all the images provided in the test_source and test_target. \n","#@markdown During this step, a threshold of 0.5 is applied on each probability map.\n","\n","warnings.filterwarnings('ignore')\n","\n","# ------------- Initialise folders ------------\n","# Create a quality control/Prediction Folder\n","#prediction_QC = os.path.join(model_save, 'Quality Control', 'Prediction')\n","#if os.path.exists(prediction_QC):\n","  #shutil.rmtree(prediction_QC)\n","#os.makedirs(prediction_QC)\n","\n","# ------------- Prepare the model and run predictions ------------\n","\n","# Load the model\n","unet = load_model( os.path.join(model_save, model_weights), custom_objects={'_weighted_binary_crossentropy': weighted_binary_crossentropy(np.ones(2))})\n","Input_size = unet.input_shape[1:3]\n","#print('Model input size: '+str(Input_size[0])+'x'+str(Input_size[1]))\n","\n","# Create a list of sources\n","source_dir_list = os.listdir(Source_QC)\n","number_of_dataset = len(source_dir_list)\n","print('Number of images found in the folder: '+str(number_of_dataset))\n","\n","\n","# Compute the metrics\n","intersection_list = []\n","union_list = []\n","iou_list = []\n","print()\n","print('Filename, Intersection, Union, IoU')\n","\n","for i in tqdm(range(number_of_dataset)):\n","  filename = source_dir_list[i]\n","  \n","  if filename.startswith('.'):\n","    print('Rejected ', file)\n","  elif filename.endswith('.tif') or filename.endswith('.tiff') or filename.endswith('.png'):\n","    prediction_image = predict_as_tiles(os.path.join(Source_QC, filename), unet)\n","    test_target_image = io.imread(os.path.join(Target_QC, filename), as_gray=True)\n","\n","    #io.imsave(os.path.join(prediction_QC, 'prediction_' + filename), prediction_image)\n","    \n","    mask_target = img_as_ubyte(test_target_image, force_copy=True)\n","    mask_prediction = img_as_ubyte(prediction_image, force_copy=True)\n","\n","    mask_target[mask_target > 0.5] = 255\n","    mask_target[mask_target <= 0.5] = 0\n","\n","    mask_prediction[mask_prediction > 128] = 255\n","    mask_prediction[mask_prediction <= 128] = 0\n","    \n","    mask_target = mask_target / 255\n","    mask_prediction = mask_prediction / 255\n","\n","    n_target = int(np.sum(mask_target))\n","    n_predic = int(np.sum(mask_prediction))\n","    intersection = np.sum(np.logical_and(mask_target, mask_prediction))\n","    union = np.sum(np.logical_or(mask_target, mask_prediction))\n","    iou_score = intersection / union\n","\n","    intersection_list.append(intersection)\n","    union_list.append(union)\n","    iou_list.append(iou_score)\n","    \n","    print(filename, intersection, union, round(iou_score,5))\n","\n","average_intersection = sum(intersection_list)/number_of_dataset\n","average_union = sum(union_list)/number_of_dataset\n","average_iou = sum(iou_list)/number_of_dataset\n","\n","print('--------------------------------------------------------------')\n","print('Average', round(average_intersection,3), round(average_union,3), round(average_iou,5))\n","print('--------------------------------------------------------------')\n"]},{"cell_type":"markdown","metadata":{"id":"9TYPdsrqfVw9"},"source":["# **7. Save your model**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"PyS-TGOw2FhU"},"outputs":[],"source":["#@markdown ### **7.1. Export the model as BMZ format**\n","\n","#@markdown This section exports the model into the BioImage Model Zoo (BMZ) format so it can be used directly with DeepImageJ. \n","\n","warnings.filterwarnings('ignore')\n","\n","# information about the model\n","Trained_model_name    = model_name\n","Trained_model_authors =  \"\" #@param {type:\"string\"}\n","Trained_model_authors_affiliation =  \"\" #@param {type:\"string\"}\n","Trained_model_description = \"Segmentation_UNet_Gothenburg2022\" \n","Trained_model_license = 'MIT'\n","Trained_model_references = [\"O. Ronneberger *et al. MICCAI 2015\", \n","                            \"L. von Chamier et al., Nature Methods, 2021\", \n","                            \"W. Ouyang et al., biorxiv  2022\",\n","                            \"E. Gómez-de-Mariscal et al., Nature Methods 202\"] \n","Trained_model_DOI = [\"https://arxiv.org/abs/1505.04597\",\n","                     \"https://www.nature.com/articles/s41467-021-22518-0\",\n","                     \"https://www.biorxiv.org/content/10.1101/2022.06.07.495102v1\",\n","                     \"https://doi.org/10.1038/s41592-021-01262-9\"] \n","\n","print('Trained_model_name: ', Trained_model_name)\n","print('Trained_model_authors: ', Trained_model_authors)\n","print('Trained_model_authors_affiliation: ', Trained_model_authors_affiliation)\n","print('Trained_model_description: ', Trained_model_description)\n","print('Trained_model_license: ', Trained_model_license)\n","print('Trained_model_references: ', Trained_model_references)\n","print('Trained_model_DOI: ', Trained_model_DOI)\n","\n","# Training data\n","include_training_data = False\n","data_from_bioimage_model_zoo = False\n","training_data_ID = ''\n","training_data_source = ''\n","training_data_description = ''\n","apply_threshold = True\n","Use_The_Best_Average_Threshold = True\n","average_best_threshold = 128\n","threshold = average_best_threshold\n","PixelSize = 1\n","#print('Mask_threshold:' threshold)\n","#print('PixelSize:' PixelSize)\n","default_example_image = True\n","#fileID    =  \"/content/gdrive/MyDrive/Projectos/DEEP-IMAGEJ/examples_of_models/ZeroCostDL4Mic/data4UNet/zhixuhao/test_images/image28.tif\" #@param {type:\"string\"}\n","\n","QC_model_folder = model_save\n","\n","if Use_The_Best_Average_Threshold:\n","    threshold = average_best_threshold\n","threshold /= 255.0\n","\n","if default_example_image:\n","    source_dir_list = os.listdir(Source_QC)\n","    fileID = os.path.join(Source_QC, source_dir_list[0])\n","\n","print()\n","print(\"------------------------------------------\")\n","print('PARAMETERS')\n","print('threshold: ', threshold)\n","print('PixelSize: ', PixelSize)\n","print('Test image: ', fileID)\n","\n","# load the model\n","compiled_weight_path = os.path.join(model_save, model_weights)\n","print('Weights: ', compiled_weight_path)\n","unet = load_model(compiled_weight_path, custom_objects={'_weighted_binary_crossentropy': weighted_binary_crossentropy(np.ones(2))})\n","print()\n","\n","# remove the custom loss function from the model, so that it can be used outside of this notebook\n","unet = Model(unet.input, unet.output)\n","weight_path = os.path.join(model_save, 'keras_weights.hdf5')\n","unet.save(weight_path)\n","\n","# create the author spec input\n","auth_names = Trained_model_authors[1:-1].split(\",\")\n","auth_affs = Trained_model_authors_affiliation[1:-1].split(\",\")\n","assert len(auth_names) == len(auth_affs)\n","authors = [{\"name\": auth_name, \"affiliation\": auth_aff} for auth_name, auth_aff in zip(auth_names, auth_affs)]\n","license = Trained_model_license\n","\n","# where to save the model\n","output_root = os.path.join(model_path, Trained_model_name + '.bioimage.io.model')\n","os.makedirs(output_root, exist_ok=True)\n","output_path = os.path.join(output_root, f\"{Trained_model_name}.zip\")\n","\n","# create a markdown readme with information\n","readme_path = os.path.join(output_root, \"README.md\")\n","with open(readme_path, \"w\") as f:\n","  f.write(\"Visit https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki\")\n","\n","# create the citation input spec\n","assert len(Trained_model_DOI) == len(Trained_model_references)\n","citations = [{'text': text, 'doi': doi} for text, doi in zip(Trained_model_references, Trained_model_DOI)]\n","\n","# create the training data\n","if include_training_data:\n","    if data_from_bioimage_model_zoo:\n","      training_data = {\"id\": training_data_ID}\n","    else:\n","      training_data = {\"source\": training_data_source,\n","                       \"description\": training_data_description}\n","else:\n","    training_data={}\n","\n","# create the input spec\n","min_percentile = 1\n","max_percentile = 99.8\n","shape = [sh.value for sh in unet.input.shape]\n","\n","# batch should never be constrained\n","assert shape[0] is None\n","shape[0] = 1  # batch is set to 1 for bioimagei.io\n","assert all(sh is not None for sh in shape)  # make sure all other shapes are fixed\n","pixel_size = {\"x\": PixelSize, \"y\": PixelSize}\n","kwargs = dict(\n","  input_names=[\"input\"],\n","  input_axes=[\"bxyc\"],\n","  pixel_sizes=[pixel_size],\n","  preprocessing=[[{\"name\": \"scale_range\", \"kwargs\": {\"min_percentile\": min_percentile, \"max_percentile\": max_percentile, \"mode\": \"per_sample\",\"axes\": \"xyc\"}}]]\n",")\n","shape = tuple(shape)\n","\n","print()\n","print(\"------------------------------------------\")\n","print('INFORMATION')\n","\n","if apply_threshold:\n","  print(\"The model output is thresholded\")\n","  postprocessing = [[{\"name\": \"binarize\", \"kwargs\": {\"threshold\": threshold}}]]\n","else:\n","  print(\"The model output is not thresholded\")\n","  postprocessing = None\n","\n","output_spec = dict(\n","  output_names=[\"output\"],\n","  output_axes=[\"bxyc\"],\n","  postprocessing=postprocessing\n",")\n","kwargs.update(output_spec)\n","\n","# load the input image, crop it if necessary and save as numpy file\n","test_img = io.imread(fileID)\n","assert test_img.ndim == 2\n","test_img = test_img[:shape[1], :shape[2]]\n","assert test_img.shape == shape[1:3], f\"{test_img.shape}, {shape}\"\n","\n","# Save the test image\n","test_in_path = os.path.join(output_root, \"test_input.npy\")\n","\n","np.save(test_in_path, test_img[None, ..., None])  # add batch and channel axis\n","# Normalize the image before adding batch and channel dimensions\n","test_img = normalizePercentile(test_img.astype(\"float32\"))\n","test_img = test_img[None, ..., None] \n","test_prediction = unet.predict(test_img)\n","\n","# run prediction on the input image and save the result as expected output\n","if apply_threshold:\n","  test_prediction = np.squeeze(test_prediction) > threshold\n","  test_prediction = test_prediction.astype(np.uint8)\n","else:\n","  test_prediction = np.squeeze(test_prediction)\n","assert test_prediction.ndim == 2\n","test_prediction = test_prediction[None, ..., None]  \n","\n","# add batch and channel axis\n","test_out_path = os.path.join(output_root, \"test_output.npy\")\n","np.save(test_out_path, test_prediction)\n","\n","# attach the QC report to the model (if it exists)\n","qc_path = os.path.join(model_save, 'Quality Control', 'training_evaluation-' + model_name + '.csv')\n","if os.path.exists(qc_path):\n","  attachments = {\"files\": [qc_path]}\n","else:\n","  attachments = None\n","\n","# export the model with keras weihgts\n","build_model(\n","    weight_uri=weight_path,\n","    test_inputs=[test_in_path],\n","    test_outputs=[test_out_path],\n","    name=Trained_model_name,\n","    description=Trained_model_description,\n","    authors=authors,\n","    tags=['zerocostdl4mic', 'deepimagej', 'segmentation', 'tem', 'unet'],\n","    license=license,\n","    documentation=readme_path,\n","    cite=citations,\n","    output_path=output_path,\n","    add_deepimagej_config=True,\n","    tensorflow_version=tf.__version__,\n","    attachments=attachments,\n","    training_data = training_data,\n","    **kwargs\n",")\n","\n","\n","# convert the keras weights to tensorflow and add them to the model\n","tf_weight_path = os.path.join(QC_model_folder, \"tf_weights\")\n","\n","# we need to make sure that the tf weight folder does not exist\n","if os.path.exists(tf_weight_path):\n","  rmtree(tf_weight_path)\n","\n","convert_weights_to_tensorflow_saved_model_bundle(output_path, tf_weight_path + \".zip\")\n","add_weights(output_path, tf_weight_path + \".zip\", output_path, tensorflow_version=tf.__version__)\n","\n","# check that the model works for keras and tensorflow \n","res = test_model(output_path, weight_format=\"keras_hdf5\")\n","success = True\n","if res[\"error\"] is not None:\n","  success = False\n","  print(\"test-model failed for keras weights:\", res[\"error\"])\n","res = test_model(output_path, weight_format=\"tensorflow_saved_model_bundle\")\n","if res[\"error\"] is not None:\n","  success = False\n","  print(\"test-model failed for tensorflow weights:\", res[\"error\"])\n","if success:\n","  print(\"The bioimage.io model was successfully exported to\", output_path)\n","else:\n","  print(\"The bioimage.io model was exported to\", output_path)\n","  print(\"Some tests of the model did not work! You can still download and test the model.\")\n","  print(\"You can still download and test the model, but it may not work as expected.\")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[{"file_id":"https://github.com/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks/BioImage.io%20notebooks/U-Net_2D_ZeroCostDL4Mic_BioImageModelZoo_export.ipynb","timestamp":1654068762467}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.12"}},"nbformat":4,"nbformat_minor":0}